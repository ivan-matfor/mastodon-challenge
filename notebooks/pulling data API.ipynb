{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed153c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STOP] Reached older than DAYS_BACK window.\n",
      "Rows collected: 2402\n"
     ]
    }
   ],
   "source": [
    "# Inspiration:\n",
    "# Syed Zainullah Qazi, \"Mastodon Data Extraction â€“ Research and Learning Purpose\"\n",
    "# https://medium.com/@syedzainullahqazi/mastodon-data-extraction-research-and-learning-purpose-1eec53068b15\n",
    "# The pagination and rate-limit handling logic was adapted and extended.\n",
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "BASE_URL = \"https://mastodon.social\"\n",
    "HASHTAG = \"ai\"  # <-- change for a different tag\n",
    "ENDPOINT = f\"/api/v1/timelines/tag/{HASHTAG}\"\n",
    "\n",
    "DAYS_BACK = 14\n",
    "LIMIT = 40                  # maximum items per request\n",
    "MAX_PAGES = 20000           # safety stop to avoid infinite loops\n",
    "DEBUG = True\n",
    "\n",
    "# If you have a token, use it (recommended). Otherwise leave as None.\n",
    "ACCESS_TOKEN = None  # \"PASTE_HERE\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"mastodon-research-scraper/1.0\",\n",
    "}\n",
    "if ACCESS_TOKEN:\n",
    "    headers[\"Authorization\"] = f\"Bearer {ACCESS_TOKEN}\"\n",
    "\n",
    "def clean_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract plain text from Mastodon HTML content.\n",
    "    \"\"\"\n",
    "    return BeautifulSoup(html or \"\", \"html.parser\").get_text(\" \", strip=True)\n",
    "\n",
    "def parse_reset_ts(reset_value: str):\n",
    "    \"\"\"\n",
    "    X-RateLimit-Reset is usually a Unix timestamp (seconds).\n",
    "    If parsing fails, return None.\n",
    "    \"\"\"\n",
    "    if not reset_value:\n",
    "        return None\n",
    "    try:\n",
    "        return float(reset_value)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def smart_throttle(resp: requests.Response, min_pause=(0.3, 0.8)):\n",
    "    \"\"\"\n",
    "    - Always apply a small random pause (anti-bot behavior smoothing)\n",
    "    - If Remaining is low, sleep until Reset\n",
    "    \"\"\"\n",
    "    # Small jitter pause\n",
    "    time.sleep(random.uniform(*min_pause))\n",
    "\n",
    "    limit_h = resp.headers.get(\"X-RateLimit-Limit\")\n",
    "    remaining_h = resp.headers.get(\"X-RateLimit-Remaining\")\n",
    "    reset_h = resp.headers.get(\"X-RateLimit-Reset\")\n",
    "\n",
    "    try:\n",
    "        remaining = int(remaining_h) if remaining_h is not None else None\n",
    "    except ValueError:\n",
    "        remaining = None\n",
    "\n",
    "    reset_ts = parse_reset_ts(reset_h)\n",
    "\n",
    "    # If close to rate limit, wait until reset (+1 second safety)\n",
    "    if remaining is not None and remaining <= 5 and reset_ts is not None:\n",
    "        wait = max(0, reset_ts - time.time()) + 1.0\n",
    "        if DEBUG:\n",
    "            print(f\"[THROTTLE] remaining={remaining}. Sleeping {wait:.1f}s until reset.\")\n",
    "        time.sleep(wait)\n",
    "\n",
    "def safe_get_json(session: requests.Session, url: str, params: dict):\n",
    "    \"\"\"\n",
    "    Perform GET request safely.\n",
    "    Handles 429 (rate limit) and non-200 responses.\n",
    "    \"\"\"\n",
    "    resp = session.get(url, params=params, headers=headers, timeout=30)\n",
    "\n",
    "    if resp.status_code == 429:\n",
    "        # Respect Retry-After header if present\n",
    "        ra = resp.headers.get(\"Retry-After\")\n",
    "        reset_ts = parse_reset_ts(resp.headers.get(\"X-RateLimit-Reset\"))\n",
    "\n",
    "        if ra is not None:\n",
    "            try:\n",
    "                wait = float(ra) + 1.0\n",
    "            except ValueError:\n",
    "                wait = 10.0\n",
    "        elif reset_ts is not None:\n",
    "            wait = max(0, reset_ts - time.time()) + 1.0\n",
    "        else:\n",
    "            wait = 30.0\n",
    "\n",
    "        if DEBUG:\n",
    "            print(f\"[429] Rate limited. Sleeping {wait:.1f}s and retrying...\")\n",
    "        time.sleep(wait)\n",
    "        return None, resp\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        if DEBUG:\n",
    "            print(f\"[HTTP {resp.status_code}] {resp.text[:200]}\")\n",
    "        return None, resp\n",
    "\n",
    "    try:\n",
    "        return resp.json(), resp\n",
    "    except ValueError:\n",
    "        if DEBUG:\n",
    "            print(\"[ERROR] Response is not JSON:\", resp.text[:200])\n",
    "        return None, resp\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "url = f\"{BASE_URL}{ENDPOINT}\"\n",
    "since = pd.Timestamp.utcnow() - pd.Timedelta(days=DAYS_BACK)\n",
    "\n",
    "session = requests.Session()\n",
    "results = []\n",
    "\n",
    "params = {\"limit\": LIMIT}\n",
    "page = 0\n",
    "\n",
    "while True:\n",
    "    page += 1\n",
    "    if page > MAX_PAGES:\n",
    "        print(\"[STOP] MAX_PAGES reached.\")\n",
    "        break\n",
    "\n",
    "    data, resp = safe_get_json(session, url, params)\n",
    "    if resp is not None:\n",
    "        smart_throttle(resp)\n",
    "\n",
    "    # If request failed (e.g. 429), retry in next loop\n",
    "    if data is None:\n",
    "        continue\n",
    "\n",
    "    if not isinstance(data, list) or len(data) == 0:\n",
    "        if DEBUG:\n",
    "            print(\"[STOP] Empty list.\")\n",
    "        break\n",
    "\n",
    "    stop = False\n",
    "    last_id = None\n",
    "\n",
    "    for toot in data:\n",
    "        last_id = toot.get(\"id\")\n",
    "\n",
    "        created_at = toot.get(\"created_at\")\n",
    "        if not created_at:\n",
    "            continue\n",
    "\n",
    "        ts = pd.to_datetime(created_at, utc=True)\n",
    "        if ts < since:\n",
    "            stop = True\n",
    "            break\n",
    "\n",
    "        acc = toot.get(\"account\") or {}\n",
    "        content = clean_text(toot.get(\"content\"))\n",
    "\n",
    "        results.append({\n",
    "            \"timestamp\": ts,\n",
    "            \"content\": content,\n",
    "            \"language\": toot.get(\"language\"),\n",
    "            \"username\": acc.get(\"username\"),\n",
    "            \"acct\": acc.get(\"acct\"),\n",
    "            \"display_name\": acc.get(\"display_name\"),\n",
    "            \"followers_count\": acc.get(\"followers_count\"),\n",
    "            \"following_count\": acc.get(\"following_count\"),\n",
    "            \"statuses_count\": acc.get(\"statuses_count\"),\n",
    "            \"visibility\": toot.get(\"visibility\"),\n",
    "            \"toot_id\": toot.get(\"id\"),\n",
    "            \"toot_url\": toot.get(\"url\"),\n",
    "            \"replies_count\": toot.get(\"replies_count\"),\n",
    "            \"reblogs_count\": toot.get(\"reblogs_count\"),\n",
    "            \"favourites_count\": toot.get(\"favourites_count\"),\n",
    "        })\n",
    "\n",
    "    if stop:\n",
    "        if DEBUG:\n",
    "            print(\"[STOP] Reached older than DAYS_BACK window.\")\n",
    "        break\n",
    "\n",
    "    if not last_id:\n",
    "        if DEBUG:\n",
    "            print(\"[STOP] No last_id for pagination.\")\n",
    "        break\n",
    "\n",
    "    # Pagination backward in time\n",
    "    params[\"max_id\"] = last_id\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "print(\"Rows collected:\", len(df))\n",
    "\n",
    "# Derived features for weekday/hour analysis\n",
    "df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "df[\"weekday\"] = df[\"timestamp\"].dt.day_name()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2eb664",
   "metadata": {},
   "source": [
    "### Multi-Tag Data Collection\n",
    "\n",
    "The extraction pipeline was executed separately for different hashtags to ensure comparable datasets.\n",
    "\n",
    "To control for temporal bias, data was collected within the same time window (`DAYS_BACK`) for each tag.  \n",
    "The resulting datasets were then exported to CSV files, effectively creating fixed snapshots for downstream comparative analysis.\n",
    "\n",
    "In this project, we analyzed:\n",
    "- `tech`\n",
    "- `cooking`\n",
    "\n",
    "These represent distinct topical communities, enabling meaningful statistical comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfdd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mastodon_dataset_yourtag.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mastodon_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
